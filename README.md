# Ollama Local Assistant

[English](#english) | [ä¸­æ–‡](#ä¸­æ–‡)

---

## English

A VS Code extension that allows you to use local Ollama AI models for code assistance directly in VS Code.

### Features

- ğŸ¤– **Code Explanation**: Select code and get detailed explanations from AI
- ğŸ”§ **Code Refactoring**: Automatically optimize and refactor selected code
- ğŸ’¡ **Code Generation**: Generate code snippets based on descriptions
- â“ **Smart Q&A**: Ask questions and get professional answers
- ğŸ“¦ **Model Management**: Easily switch between different local models

### Prerequisites

1. **Install Ollama**:
   - Visit [Ollama website](https://ollama.ai) to download and install
   - Ensure Ollama service is running (default: `http://127.0.0.1:11434`)

2. **Download Models**:
   ```bash
   # For example, download llama3 model
   ollama pull llama3
   
   # Or download other models:
   ollama pull codellama
   ollama pull mistral
   ```

### Installation

#### From VS Code Marketplace

1. Open VS Code
2. Go to Extensions view (`Ctrl+Shift+X` or `Cmd+Shift+X`)
3. Search for "Ollama Local Assistant"
4. Click Install

#### From VSIX File

1. Download the `.vsix` file
2. In VS Code, go to Extensions view
3. Click the `...` menu and select "Install from VSIX..."
4. Choose the downloaded file

### Usage

#### Configuration

Open VS Code Settings (`Ctrl+,` or `Cmd+,`), search for "Ollama", and configure:

- **Ollama Base Url**: The base URL for Ollama API (default: `http://127.0.0.1:11434`)
- **Ollama Model**: The default model name (default: `llama3`)
- **Ollama Timeout**: API request timeout in milliseconds (default: 60000)

#### Available Commands

1. **Ask Ollama** (`Ctrl+Shift+P` â†’ `Ollama: Ask Ollama`)
   - Opens input box to ask questions
   - Responses are displayed in the output panel

2. **Explain Selected Code** (`Ctrl+Shift+P` â†’ `Ollama: Explain Selected Code`)
   - Select code in the editor
   - Run command to get detailed code explanation

3. **Refactor Selected Code** (`Ctrl+Shift+P` â†’ `Ollama: Refactor Selected Code`)
   - Select code to refactor
   - AI provides optimized code
   - Option to apply changes to the file

4. **Generate Code** (`Ctrl+Shift+P` â†’ `Ollama: Generate Code`)
   - Enter code description
   - AI generates corresponding code
   - Option to insert into current file

5. **Set Model** (`Ctrl+Shift+P` â†’ `Ollama: Set Model`)
   - Select from available model list

### Development

```bash
# Install dependencies
npm install

# Compile TypeScript
npm run compile

# Watch mode compilation
npm run watch
```

### Technology Stack

- TypeScript
- VS Code Extension API
- Native Fetch API (no external HTTP dependencies)
- Ollama API

### License

MIT

### Contributing

Contributions are welcome! Please feel free to submit issues and pull requests.

---

## ä¸­æ–‡

è¿™æ˜¯ä¸€ä¸ª VS Code æ’ä»¶ï¼Œå…è®¸ä½ åœ¨ VS Code ä¸­ä½¿ç”¨æœ¬åœ°é€šè¿‡ Ollama æ­å»ºçš„å¤§æ¨¡å‹è¿›è¡Œä»£ç è¾…åŠ©ã€‚

### åŠŸèƒ½ç‰¹æ€§

- ğŸ¤– **ä»£ç è§£é‡Š**ï¼šé€‰ä¸­ä»£ç åï¼ŒAI ä¼šè¯¦ç»†è§£é‡Šä»£ç çš„åŠŸèƒ½å’ŒåŸç†
- ğŸ”§ **ä»£ç é‡æ„**ï¼šè‡ªåŠ¨ä¼˜åŒ–å’Œé‡æ„é€‰ä¸­çš„ä»£ç 
- ğŸ’¡ **ä»£ç ç”Ÿæˆ**ï¼šæ ¹æ®æè¿°ç”Ÿæˆä»£ç ç‰‡æ®µ
- â“ **æ™ºèƒ½é—®ç­”**ï¼šå‘ AI æé—®ï¼Œè·å¾—ä¸“ä¸šå›ç­”
- ğŸ“¦ **æ¨¡å‹ç®¡ç†**ï¼šè½»æ¾åˆ‡æ¢ä¸åŒçš„æœ¬åœ°æ¨¡å‹

### å‰ç½®è¦æ±‚

1. **å®‰è£… Ollama**ï¼š
   - è®¿é—® [Ollama å®˜ç½‘](https://ollama.ai) ä¸‹è½½å¹¶å®‰è£…
   - ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œï¼ˆé»˜è®¤åœ°å€ï¼š`http://127.0.0.1:11434`ï¼‰

2. **ä¸‹è½½æ¨¡å‹**ï¼š
   ```bash
   # ä¾‹å¦‚ä¸‹è½½ llama3 æ¨¡å‹
   ollama pull llama3
   
   # æˆ–ä¸‹è½½å…¶ä»–æ¨¡å‹ï¼Œå¦‚ï¼š
   ollama pull codellama
   ollama pull mistral
   ```

### å®‰è£…æ’ä»¶

#### ä» VS Code å¸‚åœºå®‰è£…

1. æ‰“å¼€ VS Code
2. è¿›å…¥æ‰©å±•è§†å›¾ (`Ctrl+Shift+X` æˆ– `Cmd+Shift+X`)
3. æœç´¢ "Ollama Local Assistant"
4. ç‚¹å‡»å®‰è£…

#### ä» VSIX æ–‡ä»¶å®‰è£…

1. ä¸‹è½½ `.vsix` æ–‡ä»¶
2. åœ¨ VS Code ä¸­ï¼Œè¿›å…¥æ‰©å±•è§†å›¾
3. ç‚¹å‡» `...` èœå•ï¼Œé€‰æ‹© "Install from VSIX..."
4. é€‰æ‹©ä¸‹è½½çš„æ–‡ä»¶

### ä½¿ç”¨æ–¹æ³•

#### é…ç½®è®¾ç½®

æ‰“å¼€ VS Code è®¾ç½®ï¼ˆ`Ctrl+,` æˆ– `Cmd+,`ï¼‰ï¼Œæœç´¢ "Ollama"ï¼Œå¯ä»¥é…ç½®ï¼š

- **Ollama Base Url**ï¼šOllama API çš„åŸºç¡€ URLï¼ˆé»˜è®¤ï¼š`http://127.0.0.1:11434`ï¼‰
- **Ollama Model**ï¼šé»˜è®¤ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼ˆé»˜è®¤ï¼š`llama3`ï¼‰
- **Ollama Timeout**ï¼šAPI è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆé»˜è®¤ï¼š60000 æ¯«ç§’ï¼‰

#### å¯ç”¨å‘½ä»¤

1. **å‘ Ollama æé—®** (`Ctrl+Shift+P` â†’ `Ollama: å‘ Ollama æé—®`)
   - æ‰“å¼€è¾“å…¥æ¡†ï¼Œè¾“å…¥é—®é¢˜
   - å“åº”ä¼šæ˜¾ç¤ºåœ¨è¾“å‡ºé¢æ¿ä¸­

2. **è§£é‡Šé€‰ä¸­ä»£ç ** (`Ctrl+Shift+P` â†’ `Ollama: è§£é‡Šé€‰ä¸­ä»£ç `)
   - åœ¨ç¼–è¾‘å™¨ä¸­é€‰ä¸­ä»£ç 
   - è¿è¡Œå‘½ä»¤ï¼ŒAI ä¼šè¯¦ç»†è§£é‡Šä»£ç 

3. **é‡æ„é€‰ä¸­ä»£ç ** (`Ctrl+Shift+P` â†’ `Ollama: é‡æ„é€‰ä¸­ä»£ç `)
   - é€‰ä¸­è¦é‡æ„çš„ä»£ç 
   - AI ä¼šæä¾›ä¼˜åŒ–åçš„ä»£ç 
   - å¯é€‰æ‹©æ˜¯å¦åº”ç”¨åˆ°æ–‡ä»¶ä¸­

4. **ç”Ÿæˆä»£ç ** (`Ctrl+Shift+P` â†’ `Ollama: ç”Ÿæˆä»£ç `)
   - è¾“å…¥ä»£ç æè¿°
   - AI ä¼šç”Ÿæˆç›¸åº”çš„ä»£ç 
   - å¯é€‰æ‹©æ˜¯å¦æ’å…¥åˆ°å½“å‰æ–‡ä»¶

5. **è®¾ç½®ä½¿ç”¨çš„æ¨¡å‹** (`Ctrl+Shift+P` â†’ `Ollama: è®¾ç½®ä½¿ç”¨çš„æ¨¡å‹`)
   - ä»å¯ç”¨æ¨¡å‹åˆ—è¡¨ä¸­é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹

### å¼€å‘

```bash
# å®‰è£…ä¾èµ–
npm install

# ç¼–è¯‘ TypeScript
npm run compile

# ç›‘å¬æ¨¡å¼ç¼–è¯‘
npm run watch
```

### æŠ€æœ¯æ ˆ

- TypeScript
- VS Code Extension API
- åŸç”Ÿ Fetch APIï¼ˆæ— å¤–éƒ¨ HTTP ä¾èµ–ï¼‰
- Ollama API

### è®¸å¯è¯

MIT
å¯ä»¥å…è´¹ä½¿ç”¨ï¼Œä½†éœ€è¦æ³¨æ˜åŸå§‹æ¥æº

### è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼
[æŸ¥çœ‹GitHubä»“åº“](https://github.com/zhangming9502/ollama-local-assistant/tree/master)